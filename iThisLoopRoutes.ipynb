{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boring Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6501024</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65050465</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">650982</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">650437</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6501054</span>\n",
       "         <span style=\"color: #808000; text-decoration-color: #808000\">...</span>    \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">541</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">216800026</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">542</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">236500026</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">543</span>            <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">544</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">246500008</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">545</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8912</span>\n",
       "Name: CUST, Length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">546</span>, dtype: object\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m0\u001b[0m        \u001b[1;36m6501024\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m       \u001b[1;36m65050465\u001b[0m\n",
       "\u001b[1;36m2\u001b[0m         \u001b[1;36m650982\u001b[0m\n",
       "\u001b[1;36m3\u001b[0m         \u001b[1;36m650437\u001b[0m\n",
       "\u001b[1;36m4\u001b[0m        \u001b[1;36m6501054\u001b[0m\n",
       "         \u001b[33m...\u001b[0m    \n",
       "\u001b[1;36m541\u001b[0m    \u001b[1;36m216800026\u001b[0m\n",
       "\u001b[1;36m542\u001b[0m    \u001b[1;36m236500026\u001b[0m\n",
       "\u001b[1;36m543\u001b[0m            \u001b[1;36m1\u001b[0m\n",
       "\u001b[1;36m544\u001b[0m    \u001b[1;36m246500008\u001b[0m\n",
       "\u001b[1;36m545\u001b[0m         \u001b[1;36m8912\u001b[0m\n",
       "Name: CUST, Length: \u001b[1;36m546\u001b[0m, dtype: object\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Updating coordinates<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Updating coordinates\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353/353 [00:00<00:00, 2069.23it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot call `vectorize` on size 0 inputs unless `otypes` is set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1346\u001b[0m\n\u001b[1;32m   1340\u001b[0m distance_matrix \u001b[38;5;241m=\u001b[39m create_distance_matrix(dataset)\n\u001b[1;32m   1341\u001b[0m latlong_df \u001b[38;5;241m=\u001b[39m dataset[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m-> 1346\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW DAY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNEW DAY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_replace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[^\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md]+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# Convert the cleaned 'NEW DAY' values from strings to integers\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;66;03m# Note: This will convert empty strings to NaN, which you might need to handle based on your use case\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW DAY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\n\u001b[1;32m   1352\u001b[0m     dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW DAY\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/generic.py:8093\u001b[0m, in \u001b[0;36mNDFrame.replace\u001b[0;34m(self, to_replace, value, inplace, limit, regex, method)\u001b[0m\n\u001b[1;32m   8088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(to_replace) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value):\n\u001b[1;32m   8089\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   8090\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReplacement lists must match in length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   8091\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(to_replace)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   8092\u001b[0m         )\n\u001b[0;32m-> 8093\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8094\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_replace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdest_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8096\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8098\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m to_replace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   8102\u001b[0m         is_re_compilable(regex)\n\u001b[1;32m   8103\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_list_like(regex)\n\u001b[1;32m   8104\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_dict_like(regex)\n\u001b[1;32m   8105\u001b[0m     ):\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/internals/base.py:278\u001b[0m, in \u001b[0;36mDataManager.replace_list\u001b[0;34m(self, src_list, dest_list, inplace, regex)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"do a list replace\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 278\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace_list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdest_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_AlreadyWarned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m bm\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:364\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    367\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1101\u001b[0m, in \u001b[0;36mBlock.replace_list\u001b[0;34m(self, src_list, dest_list, inplace, regex, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         already_warned\u001b[38;5;241m.\u001b[39mwarned_already \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m opt \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuture.no_silent_downcasting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ((src, dest), mask) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(pairs, masks)):\n\u001b[1;32m   1102\u001b[0m     convert \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m==\u001b[39m src_len  \u001b[38;5;66;03m# only convert once at the end\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m     new_rb: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1064\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_string_dtype(values\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;66;03m# Calculate the mask once, prior to the call of comp\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;66;03m# in order to avoid repeating the same computations\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m     na_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39misna(values)\n\u001b[1;32m   1060\u001b[0m     masks: Iterable[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_]] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m         extract_bool_array(\n\u001b[1;32m   1062\u001b[0m             cast(\n\u001b[1;32m   1063\u001b[0m                 ArrayLike,\n\u001b[0;32m-> 1064\u001b[0m                 \u001b[43mcompare_or_regex_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_mask\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1067\u001b[0m             )\n\u001b[1;32m   1068\u001b[0m         )\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pairs\n\u001b[1;32m   1070\u001b[0m     )\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;66;03m# GH#38086 faster if we know we dont need to check for regex\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m     masks \u001b[38;5;241m=\u001b[39m (missing\u001b[38;5;241m.\u001b[39mmask_missing(values, s[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pairs)\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/pandas/core/array_algos/replace.py:98\u001b[0m, in \u001b[0;36mcompare_or_regex_search\u001b[0;34m(a, b, regex, mask)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     96\u001b[0m     a \u001b[38;5;241m=\u001b[39m a[mask]\n\u001b[0;32m---> 98\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# The shape of the mask can differ to that of the result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# since we may compare only a subset of a's or b's elements\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(mask\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2372\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stage_2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 2372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_as_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2365\u001b[0m, in \u001b[0;36mvectorize._call_as_normal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[1;32m   2363\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectorize_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2450\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     res \u001b[38;5;241m=\u001b[39m func()\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     ufunc, otypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ufunc_and_otypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[1;32m   2453\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/kg/drive/.conda/lib/python3.10/site-packages/numpy/lib/function_base.py:2406\u001b[0m, in \u001b[0;36mvectorize._get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2404\u001b[0m args \u001b[38;5;241m=\u001b[39m [asarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builtins\u001b[38;5;241m.\u001b[39many(arg\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args):\n\u001b[0;32m-> 2406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcannot call `vectorize` on size 0 inputs \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2407\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munless `otypes` is set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2409\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [arg\u001b[38;5;241m.\u001b[39mflat[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m   2410\u001b[0m outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot call `vectorize` on size 0 inputs unless `otypes` is set"
     ]
    }
   ],
   "source": [
    "# The Beginning\n",
    "# Define the functios and load the data.\n",
    "import collections\n",
    "from rich import inspect\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "import time\n",
    "import csv\n",
    "from IPython.display import Image, display\n",
    "import datetime\n",
    "import io\n",
    "import math\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from rich import print\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import track\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich import box\n",
    "from rich import color\n",
    "from folium import FeatureGroup, Tooltip\n",
    "from folium.features import DivIcon\n",
    "from folium.plugins import FastMarkerCluster, Fullscreen\n",
    "from haversine import haversine, Unit\n",
    "from IPython.display import HTML\n",
    "# import display for terminal to show the map\n",
    "from IPython.display import display\n",
    "from math import ceil\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from ortools.constraint_solver import pywrapcp, routing_enums_pb2\n",
    "from regex import F\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from scipy.spatial import ConvexHull\n",
    "from tqdm import tqdm\n",
    "from urllib import response\n",
    "from urllib.parse import quote_plus\n",
    "import folium\n",
    "import os\n",
    "import webbrowser\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from PIL import Image, ImageSequence\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np  # For colormap or random color generation\n",
    "# Function to convert HTML maps to images\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "# Prime the Solver With a First Run\n",
    "from time import sleep\n",
    "import weakref\n",
    "from IPython.core.display import HTML\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.layout import Layout\n",
    "from rich.columns import Columns\n",
    "from rich.panel import Panel\n",
    "\n",
    "console = Console()\n",
    "table = Table()\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# # FunctionF Definitions\n",
    "EARTH_RADIUS = 6371.0\n",
    "\n",
    "\n",
    "def create_final_route_frame(dataset, output_directory='route_snapshots'):\n",
    "    \"\"\"\n",
    "    Creates the final frame of a route visualization.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): A DataFrame containing route data with columns:\n",
    "            'Latitude', 'Longitude', 'NEW RT', 'NEW DAY', 'NEW STOP'\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The Matplotlib figure object representing the final frame.\n",
    "    \"\"\"\n",
    "    sorted_dataset = dataset.sort_values(by=['NEW DAY', 'NEW STOP'])\n",
    "    # Make sure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Assign a unique color to each route\n",
    "    unique_routes = sorted_dataset['NEW RT'].unique()\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_routes)))\n",
    "    route_colors = {rt: clr for rt, clr in zip(unique_routes, colors)}\n",
    "\n",
    "    # Determine global latitude and longitude ranges\n",
    "    lat_min, lat_max = sorted_dataset['Latitude'].min(\n",
    "    ), sorted_dataset['Latitude'].max()\n",
    "    lon_min, lon_max = sorted_dataset['Longitude'].min(\n",
    "    ), sorted_dataset['Longitude'].max()\n",
    "\n",
    "    # Initialize a dictionary to store cumulative route points\n",
    "    cumulative_route_points = {rt: pd.DataFrame(\n",
    "        columns=['Latitude', 'Longitude']) for rt in unique_routes}\n",
    "\n",
    "    # Extract the last day and stop\n",
    "    # last_day, last_stop = sorted_dataset.iloc[-1][['NEW DAY', 'NEW STOP']]\n",
    "\n",
    "   # Get max values for day and stop for the final frame\n",
    "    max_day = sorted_dataset['NEW DAY'].max()\n",
    "    max_stop = sorted_dataset['NEW STOP'].max()\n",
    "\n",
    "    # Create the final figure\n",
    "    plt.figure(figsize=[8, 6])\n",
    "\n",
    "    for rt in unique_routes:\n",
    "        # Get all data for the route\n",
    "        rt_data = sorted_dataset[sorted_dataset['NEW RT'] == rt]\n",
    "        if not rt_data.empty:\n",
    "            cumulative_route_points[rt] = rt_data[[\n",
    "                'Latitude', 'Longitude']].drop_duplicates()  # Cumulative points\n",
    "            plt.plot(cumulative_route_points[rt]['Longitude'], cumulative_route_points[rt]['Latitude'],\n",
    "                     '-o', color=route_colors[rt], markersize=5, linewidth=2, label=f'Route {rt}')\n",
    "    now = datetime.datetime.now()\n",
    "    plt.title(f'Final Routes {now}')  # Title for final frame\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.xlim(lon_min - 0.05, lon_max + 0.05)\n",
    "    plt.ylim(lat_min - 0.05, lat_max + 0.05)\n",
    "    # plt.legend()\n",
    "    now = datetime.datetime.now()\n",
    "    filename = os.path.join(output_directory, f'route_snapshot_{now}.png')\n",
    "    plt.savefig(filename)\n",
    "    # plt.show()\n",
    "\n",
    "    plt.close()\n",
    "    return filename  # Return the filename of the saved image\n",
    "\n",
    "\n",
    "def create_video_from_filenames(filenames, output_filename='route_animation.mp4', fps=10):\n",
    "    \"\"\"\n",
    "    Creates a video from a list of image filenames.\n",
    "\n",
    "    Args:\n",
    "        filenames (list of str): A list of filenames of the images to include in the video.\n",
    "        output_filename (str): The filename for the video to be saved as.\n",
    "        fps (int): Frames per second for the video.\n",
    "    \"\"\"\n",
    "    import imageio\n",
    "\n",
    "    # Create the MP4 animation\n",
    "    with imageio.get_writer(output_filename, fps=fps) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    print(f'Video saved as {output_filename}')\n",
    "\n",
    "\n",
    "def create_route_animation(dataset, output_filename='route_day_stop_animation.mp4'):\n",
    "    \"\"\"\n",
    "    Creates an animated map visualization of routes over time.\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): A DataFrame containing route data with columns:\n",
    "            'Latitude', 'Longitude', 'NEW RT', 'NEW DAY', 'NEW STOP'\n",
    "        output_filename (str, optional): The filename for the MP4 output. \n",
    "                \"\"\"\n",
    "    sorted_dataset = dataset.sort_values(by=['NEW DAY', 'NEW STOP'])\n",
    "    # Assign a unique color to each route\n",
    "    unique_routes = sorted_dataset['NEW RT'].unique()\n",
    "    # Change colormap as needed\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_routes)))\n",
    "    route_colors = {rt: clr for rt, clr in zip(unique_routes, colors)}\n",
    "\n",
    "    #  latitude and longitude ranges\n",
    "    lat_min, lat_max = sorted_dataset['Latitude'].min(\n",
    "    ), sorted_dataset['Latitude'].max()\n",
    "    lon_min, lon_max = sorted_dataset['Longitude'].min(\n",
    "    ), sorted_dataset['Longitude'].max()\n",
    "\n",
    "    # Initialize a dictionary to store cumulative route points\n",
    "    cumulative_route_points = {rt: pd.DataFrame(\n",
    "        columns=['Latitude', 'Longitude']) for rt in unique_routes}\n",
    "\n",
    "    filenames = []  # To store the names of the plot files\n",
    "\n",
    "    # Iterate through each day and stop\n",
    "    for (day, stop), group in sorted_dataset.groupby(['NEW DAY', 'NEW STOP']):\n",
    "        plt.figure(figsize=[8, 6])  # Set figure size as needed\n",
    "\n",
    "        for rt in unique_routes:\n",
    "            # Filter for current route up to the current stop\n",
    "            rt_data = sorted_dataset[(sorted_dataset['NEW RT'] == rt) & (\n",
    "                sorted_dataset['NEW DAY'] <= day) & (sorted_dataset['NEW STOP'] <= stop)]\n",
    "            if not rt_data.empty:\n",
    "                cumulative_route_points[rt] = pd.concat([cumulative_route_points[rt], rt_data[[\n",
    "                                                        'Latitude', 'Longitude']]], ignore_index=True).drop_duplicates()\n",
    "                plt.plot(cumulative_route_points[rt]['Longitude'], cumulative_route_points[rt]['Latitude'],\n",
    "                         '-o', color=route_colors[rt], markersize=5, linewidth=2, label=f'Route {rt}')\n",
    "\n",
    "        plt.title(f'Up to Day {day}, Stop {stop}')\n",
    "        plt.xlabel('Longitude')\n",
    "        plt.ylabel('Latitude')\n",
    "        plt.xlim(lon_min - 0.01, lon_max + 0.01)\n",
    "        plt.ylim(lat_min - 0.01, lat_max + 0.01)\n",
    "        plt.legend()\n",
    "\n",
    "        # Save the frame\n",
    "        filename = f'map_day{day}_stop{stop}.png'\n",
    "        plt.savefig(filename)\n",
    "        filenames.append(filename)\n",
    "        plt.close()\n",
    "\n",
    "    # Create the MP4 animation\n",
    "    # Adjust FPS as needed\n",
    "    with imageio.get_writer('route_day_stop_animation.mp4', fps=14) as writer:\n",
    "        for filename in filenames:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)\n",
    "\n",
    "    # Optionally, clean up the individual PNG files\n",
    "    for filename in filenames:\n",
    "        os.remove(filename)\n",
    "\n",
    "    # To display the video in Jupyter Notebook\n",
    "    video_path = 'route_day_stop_animation.mp4'\n",
    "    display(HTML(f\"\"\"\n",
    "        <video controls>\n",
    "            <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        </video>\n",
    "        \"\"\")\n",
    "            )\n",
    "\n",
    "\n",
    "def html_to_image(html_files, output_dir='maps_images', width=800, height=600):\n",
    "    # Setup Selenium webdriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(f'--window-size={width},{height}')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    image_files = []\n",
    "\n",
    "    for html_file in html_files:\n",
    "        # Open the HTML file\n",
    "        driver.get(f\"file://{os.path.abspath(html_file)}\")\n",
    "        # time.sleep(1)  # Wait for the map to load\n",
    "\n",
    "        # Define the output image path\n",
    "        output_image = f\"{output_dir}/{os.path.basename(html_file).replace('.html', '.png')}\"\n",
    "        image_files.append(output_image)\n",
    "\n",
    "        # Take screenshot\n",
    "        driver.save_screenshot(output_image)\n",
    "\n",
    "    driver.quit()\n",
    "    return image_files\n",
    "\n",
    "\n",
    "def create_animation(image_files, output_file='map_animation.gif', duration=1000):\n",
    "    images = [Image.open(image_file).convert(\n",
    "        \"P\", palette=Image.ADAPTIVE) for image_file in image_files]\n",
    "    images[0].save(output_file, save_all=True,\n",
    "                   append_images=images[1:], duration=duration, loop=0)\n",
    "\n",
    "\n",
    "def routes_by_days_to_html(dataset):\n",
    "    # Starting the HTML content\n",
    "    html_content = \"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "        table {\n",
    "            width: 100%;\n",
    "            border-collapse: collapse;\n",
    "            table-layout: fixed; /* Add fixed table layout */\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid black;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "            overflow: hidden; /* Ensures the content fits the cell */\n",
    "            word-wrap: break-word; /* Ensures words do not exceed the cell width */\n",
    "        }\n",
    "        th {\n",
    "            background-color: #f2f2f2;\n",
    "        }\n",
    "    </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "    # Get unique routes from the dataset\n",
    "    unique_routes = sorted(dataset[\"NEW RT\"].unique())\n",
    "\n",
    "    # Iterate over each unique route\n",
    "    for route in unique_routes:\n",
    "        html_content += f\"<h2>Route {route} Summary:</h2>\"\n",
    "\n",
    "        # Subset the data for the current route\n",
    "        subset_data = dataset[dataset[\"NEW RT\"] == route].sort_values(\n",
    "            [\"NEW DAY\", \"NEW STOP\"]\n",
    "        )\n",
    "\n",
    "        # Find the maximum number of days for this route\n",
    "        max_day = subset_data[\"NEW DAY\"].max()\n",
    "\n",
    "        # Iterate over each group of days\n",
    "        num_groups = ceil(max_day / 5)\n",
    "        for group in range(num_groups):\n",
    "            day_start = group * 5 + 1\n",
    "            day_end = min((group + 1) * 5, max_day)\n",
    "            day_headers = [\n",
    "                f\"Day {day_num}\" for day_num in range(day_start, day_start + 5)\n",
    "            ]  # Ensure 5 headers per group\n",
    "\n",
    "            # Start a new table for each group of days\n",
    "            html_content += (\n",
    "                \"<table><tr>\"\n",
    "                + \"\".join([f\"<th>{header}</th>\" for header in day_headers])\n",
    "                + \"</tr>\"\n",
    "            )\n",
    "\n",
    "            # Determine the maximum number of stops in any day within this group for formatting\n",
    "            max_stops = 0\n",
    "            for day_num in range(day_start, day_end + 1):\n",
    "                day_length = subset_data[subset_data[\"NEW DAY\"]\n",
    "                                         == day_num].shape[0]\n",
    "                if day_length > max_stops:\n",
    "                    max_stops = day_length\n",
    "\n",
    "            # Print customer names for each day in the group\n",
    "            for stop_num in range(1, max_stops + 1):\n",
    "                html_content += \"<tr>\"\n",
    "                # Include empty cells if fewer than 5 days in the group\n",
    "                for day_num in range(day_start, day_start + 5):\n",
    "                    day_data = subset_data[subset_data[\"NEW DAY\"] == day_num]\n",
    "                    if stop_num <= day_data.shape[0]:\n",
    "                        cust_name = (\n",
    "                            day_data.iloc[stop_num - 1][\"CUST NAME\"]\n",
    "                            if stop_num <= day_data.shape[0]\n",
    "                            else \"\"\n",
    "                        )\n",
    "                        html_content += f\"<td>{stop_num}. {cust_name}</td>\"\n",
    "                    else:\n",
    "                        # Empty cell for days with fewer stops or if the day does not exist\n",
    "                        html_content += \"<td></td>\"\n",
    "                html_content += \"</tr>\"\n",
    "            html_content += \"</table><br>\"  # Close the table and add a space\n",
    "\n",
    "    html_content += \"</body></html>\"\n",
    "\n",
    "    # Writing the HTML content to a file\n",
    "    with open(\"routes_summary.html\", \"w\") as html_file:\n",
    "        html_file.write(html_content)\n",
    "    m = HTML(html_content)\n",
    "    # display(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def print_route_statistics(dataset):\n",
    "    # Group the data by 'NEW RT' and calculate the required statistics\n",
    "    route_stats = (\n",
    "        dataset.groupby(\"NEW RT\")\n",
    "        .agg(\n",
    "            total_BIG_5_AVG=(\"BIG 5 AVG\", \"sum\"),\n",
    "            total_COMP_AVG=(\"COMP AVG\", \"sum\"),\n",
    "            number_of_NEW_DAY=(\"NEW DAY\", \"nunique\"),\n",
    "            number_of_NEW_STOP=(\"NEW STOP\", \"count\"),\n",
    "            average_NEW_DIST=(\"NEW DIST\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate Average Stops per day for each route\n",
    "    route_stats[\"average_stops_per_day\"] = (\n",
    "        route_stats[\"number_of_NEW_STOP\"] / route_stats[\"number_of_NEW_DAY\"]\n",
    "    )\n",
    "\n",
    "    # Print the statistics in a user-friendly way\n",
    "    for _, row in route_stats.iterrows():\n",
    "        print(f\"Route {int(row['NEW RT'])}:\", (\"***\" * 35))\n",
    "        print(\n",
    "            f\"  BIG 5:\\t${row['total_BIG_5_AVG']:,.2f}\\tRT Days:\\t{int(row['number_of_NEW_DAY'])}\\tStops/Day:\\t{row['average_stops_per_day']:.1f}\\tBig 5/Stop:\\t${row['total_BIG_5_AVG']/row['number_of_NEW_STOP']:,.2f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  COMP: \\t${row['total_COMP_AVG']:,.2f}\\t# of Stops:\\t{int(row['number_of_NEW_STOP'])}\\tDist/Day:\\t{row['average_NEW_DIST']:.1f}\\tComp/Stop:\\t${row['total_COMP_AVG']/row['number_of_NEW_STOP']:,.2f}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "def autosize_columns(worksheet):\n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(cell.value)\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = max_length + 21\n",
    "        worksheet.column_dimensions[cell.column_letter].width = adjusted_width\n",
    "    return worksheet\n",
    "\n",
    "\n",
    "def create_excel_by_routes(dataset):\n",
    "    # now create the\n",
    "\n",
    "    # Create a new Excel workbook and remove the default sheet\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)\n",
    "\n",
    "    # Get unique routes from the dataset\n",
    "    unique_routes = dataset[\"NEW RT\"].unique()\n",
    "\n",
    "    # Iterate over each unique route\n",
    "    for route in unique_routes:\n",
    "        # Create a new sheet for this route\n",
    "        ws = wb.create_sheet(title=f\"Route {route}\")\n",
    "\n",
    "        # Subset the data for the current route\n",
    "        subset_data = dataset[dataset[\"NEW RT\"] == route].sort_values(\n",
    "            [\"NEW DAY\", \"NEW STOP\"]\n",
    "        )\n",
    "\n",
    "        # Find the maximum number of days for this route to define how many headers we need\n",
    "        max_day = subset_data[\"NEW DAY\"].max()\n",
    "        # Calculate the number of header groups (Days 1-5, 6-10, etc.)\n",
    "        num_groups = ceil(max_day / 5)\n",
    "\n",
    "        # Set the initial row\n",
    "        start_row = 1\n",
    "\n",
    "        # Iterate over each group of days\n",
    "        for group in range(num_groups):\n",
    "            # Calculate the range of days for this group\n",
    "            day_start = group * 5 + 1\n",
    "            day_end = min((group + 1) * 5, max_day)\n",
    "\n",
    "            # Write the day headers for this group\n",
    "            for day_num in range(day_start, day_end + 1):\n",
    "                col = day_num - group * 5\n",
    "                ws.cell(row=start_row, column=col, value=f\"Day {day_num}\")\n",
    "\n",
    "            # Populate the customer names under each day\n",
    "            for day_num in range(day_start, day_end + 1):\n",
    "                # Filter the subset for this day\n",
    "                day_data = subset_data[subset_data[\"NEW DAY\"] == day_num]\n",
    "                # Write each customer name in this day's column\n",
    "                row_num = start_row + 1  # Starting row for customer names\n",
    "                for _, row in day_data.iterrows():\n",
    "                    col = day_num - group * 5  # Adjust column based on group\n",
    "                    ws.cell(row=row_num, column=col, value=row[\"CUST NAME\"])\n",
    "                    row_num += 1  # Move to the next row for the next customer\n",
    "\n",
    "            # Update the starting row for the next group of days\n",
    "            if not day_data.empty:\n",
    "                # Add extra space before the next group\n",
    "                start_row += len(day_data) + 2\n",
    "        ws = autosize_columns(ws)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(\"routes_by_days.xlsx\")\n",
    "\n",
    "\n",
    "def calculate_avg_diff(dataset):\n",
    "    avg_diff = 0\n",
    "    for i in range(len(dataset)):\n",
    "        avg_diff += abs(dataset[\"DAY\"].iloc[i] - dataset[\"NEW DAY\"].iloc[i])\n",
    "    avg_diff /= len(dataset)\n",
    "    return avg_diff\n",
    "\n",
    "\n",
    "def order_similar_to_prev(dataset):\n",
    "    # Step 1: Calculate the average 'DAY' for each 'NEW RT' and 'NEW DAY' combination\n",
    "    grouped_data = dataset.groupby([\"NEW RT\", \"NEW DAY\"])[\n",
    "        \"DAY\"].mean().reset_index()\n",
    "    grouped_data = grouped_data.rename(columns={\"DAY\": \"AVERAGE_DAY\"})\n",
    "\n",
    "    # Step 2: Sort these averages for each 'NEW RT' and reassign 'NEW DAY'\n",
    "    grouped_data.sort_values([\"NEW RT\", \"AVERAGE_DAY\"], inplace=True)\n",
    "    grouped_data[\"NEW_DAY_SORTED\"] = (\n",
    "        grouped_data.groupby(\"NEW RT\").cumcount() + 1\n",
    "    )  # Start from 1\n",
    "\n",
    "    # Merge the sorted 'NEW DAY' back to the original dataset\n",
    "    dataset = dataset.merge(\n",
    "        grouped_data[[\"NEW RT\", \"NEW DAY\", \"NEW_DAY_SORTED\"]],\n",
    "        on=[\"NEW RT\", \"NEW DAY\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    dataset[\"NEW DAY\"] = dataset[\"NEW_DAY_SORTED\"]\n",
    "    dataset.drop(\"NEW_DAY_SORTED\", axis=1, inplace=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def print_starts_ends(data, dataset):\n",
    "    for start in data[\"starts\"]:\n",
    "        print(dataset.iloc[start][\"CUST NAME\"])\n",
    "    for endd in data[\"ends\"]:\n",
    "        print(dataset.iloc[endd][\"CUST NAME\"])\n",
    "\n",
    "\n",
    "def best_tracker(solution, filename):\n",
    "    current_obj = solution.ObjectiveValue()\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                prev_obj = int(lines[-1])\n",
    "                best_obj = int(lines[-2])\n",
    "            else:\n",
    "                prev_obj = current_obj\n",
    "                best_obj = current_obj\n",
    "    except FileNotFoundError:\n",
    "        prev_obj = current_obj\n",
    "        best_obj = current_obj\n",
    "\n",
    "    # Create a table object\n",
    "    table = Table(title=\"Best Tracker\", show_lines=True)\n",
    "\n",
    "    # Add columns to the table\n",
    "    table.add_column(\"Solution\", style=\"magenta\")\n",
    "    table.add_column(\"Distance\", style=\"cyan\")\n",
    "    scurrent_obj = \"{:,}\".format(current_obj)\n",
    "    sbest_obj = \"{:,}\".format(best_obj)\n",
    "    sprev_obj = \"{:,}\".format(prev_obj)\n",
    "\n",
    "    # Add rows to the table\n",
    "    table.add_row(\"Current\", scurrent_obj)\n",
    "    table.add_row(\"Best\", sbest_obj)\n",
    "    table.add_row(\"Last Run\", sprev_obj)\n",
    "\n",
    "    if best_obj > current_obj:\n",
    "        improvement = best_obj - current_obj\n",
    "        best_obj = current_obj\n",
    "        sbest_obj = \"{:,}\".format(best_obj)\n",
    "        simprovement = \"{:,}\".format(improvement)\n",
    "\n",
    "        table.add_row(\"WINNER!\", sbest_obj)\n",
    "        table.add_row(\"IMPROVEMENT AMOUNT\", str(simprovement))\n",
    "\n",
    "    else:\n",
    "        if prev_obj > current_obj:\n",
    "            improvement = prev_obj - current_obj\n",
    "            simprovement = \"{:,}\".format(improvement)\n",
    "            table.add_row(\"IMPROVEMENT AMOUNT\", str(simprovement))\n",
    "\n",
    "        else:\n",
    "            deterioration = current_obj - prev_obj\n",
    "            sdeterioration = \"{:,}\".format(deterioration)\n",
    "\n",
    "            table.add_row(\"WORSE BY\", str(sdeterioration))\n",
    "    print(table)\n",
    "\n",
    "    # After processing, update the file with the current and new best solutions\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"{best_obj}\\n{current_obj}\\n\")\n",
    "\n",
    "    return prev_obj, best_obj\n",
    "\n",
    "\n",
    "def set_days_for_routes(\n",
    "    dataset, min_stops=6, max_stops=10, distance_threshold=80, day_factor=2\n",
    "):\n",
    "    # Get unique routes from the dataset\n",
    "    unique_routes = dataset[\"NEW RT\"].unique()\n",
    "\n",
    "    # Iterate over each unique route\n",
    "    for route in unique_routes:\n",
    "        # Subset the data for the current route, sorted by NEW STOP to ensure order\n",
    "        subset_data = dataset[dataset[\"NEW RT\"]\n",
    "                              == route].sort_values(\"NEW STOP\")\n",
    "\n",
    "        # Initialize variables for splitting into days\n",
    "        current_day = 1\n",
    "        current_stop_in_day = 1\n",
    "        cumulative_distance = 0  # Continue tracking but not using for decision\n",
    "        stops_in_current_day = []\n",
    "\n",
    "        # Iterate over rows in the sorted subset data\n",
    "        for i, row in subset_data.iterrows():\n",
    "            # Add the stop to the current day's list\n",
    "            stops_in_current_day.append(i)\n",
    "            # Update the cumulative distance but not used for decision\n",
    "            cumulative_distance += row[\"NEW DIST\"]\n",
    "\n",
    "            # Check for splitting conditions based on stop count, individual distances, and cluster threshold\n",
    "            if len(stops_in_current_day) >= min_stops and (\n",
    "                len(stops_in_current_day) >= max_stops\n",
    "                or row[\"NEW DIST\"] > distance_threshold\n",
    "                or cumulative_distance > day_factor * distance_threshold\n",
    "            ):  # Added cluster threshold check for splitting\n",
    "\n",
    "                # Update the NEW DAY and NEW STOP values in the original dataset\n",
    "                for stop_index in stops_in_current_day:\n",
    "                    dataset.at[stop_index, \"NEW DAY\"] = current_day\n",
    "                    dataset.at[stop_index, \"NEW STOP\"] = current_stop_in_day\n",
    "                    current_stop_in_day += (\n",
    "                        1  # Increment the stop number for the current day\n",
    "                    )\n",
    "\n",
    "                # Reset variables for the new day\n",
    "                current_day += 1\n",
    "                stops_in_current_day = []\n",
    "                cumulative_distance = 0  # Reset but keep for tracking\n",
    "                current_stop_in_day = 1\n",
    "\n",
    "        # Check if there are remaining stops for the last day\n",
    "        if stops_in_current_day:\n",
    "            for stop_index in stops_in_current_day:\n",
    "                dataset.at[stop_index, \"NEW DAY\"] = current_day\n",
    "                dataset.at[stop_index, \"NEW STOP\"] = current_stop_in_day\n",
    "                current_stop_in_day += 1\n",
    "\n",
    "    # Return the updated dataset\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def print_solution(data, manager, routing, solution, dataset):\n",
    "    total_distance = 0  # Total distance covered by all the vehicles\n",
    "    total_load = 0  # Total load carried by all the vehicles\n",
    "    total_load2 = 0  # Total load carried by all the vehicles\n",
    "    previous_index = routing.Start(0)\n",
    "    new_vehicle_assignment = (\n",
    "        {}\n",
    "    )  # Initialize a new dictionary to track vehicle assignments\n",
    "\n",
    "    for vehicle_id in range(data[\"num_vehicles\"]):\n",
    "        index = routing.Start(vehicle_id)\n",
    "        route_distance = 0  # Distance covered by the current vehicle\n",
    "        route_load = 0  # Load carried by the current vehicle\n",
    "        route_load2 = 0  # Load carried by the current vehicle\n",
    "        route_count = 0  # Count of stops made by the current vehicle\n",
    "        while not routing.IsEnd(index):\n",
    "            route_count += 1\n",
    "            node_index = manager.IndexToNode(index)  #\n",
    "            route_load += data[\"demands\"][node_index]\n",
    "            route_load2 += data[\"demands2\"][node_index]\n",
    "            new_vehicle_assignment[node_index] = (\n",
    "                # Assign vehicle to new_vehicle_assignment instead of data['Vehicle']\n",
    "                vehicle_id\n",
    "            )\n",
    "            previous_index = index\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            route_distance += routing.GetArcCostForVehicle(\n",
    "                previous_index, index, vehicle_id\n",
    "            )\n",
    "        end_node_index = manager.IndexToNode(\n",
    "            index\n",
    "        )  # Convert the final routing index to your node index\n",
    "        # Since 'index' at this point should be the end node for the current vehicle\n",
    "        # Assign this end node to the corresponding vehicle\n",
    "        new_vehicle_assignment[end_node_index] = (\n",
    "            vehicle_id  # Assign the route to the end node\n",
    "        )\n",
    "        route_count += 1\n",
    "        route_load += data[\"demands\"][node_index]\n",
    "        route_load2 += data[\"demands2\"][node_index]\n",
    "        new_vehicle_assignment[node_index] = (\n",
    "            # Assign vehicle to new_vehicle_assignment instead of data['Vehicle']\n",
    "            vehicle_id\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"Route {} has {} stops ({})\\nCompanion: ${:,.0f} ({})    Big 5: ${:,.0f} ({})\\nDistance: {}m\".format(\n",
    "                vehicle_id + 1,\n",
    "                route_count,\n",
    "                data[\"vehicle_capacities3\"][vehicle_id],\n",
    "                route_load,\n",
    "                data[\"vehicle_capacities\"][vehicle_id],\n",
    "                route_load2,\n",
    "                data[\"vehicle_capacities2\"][vehicle_id],\n",
    "                route_distance,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        total_distance += route_distance\n",
    "        total_load += (\n",
    "            route_load  # Add the load carried by the vehicle to the total load\n",
    "        )\n",
    "        total_load2 += (\n",
    "            route_load2  # Add the load carried by the vehicle to the total load\n",
    "        )\n",
    "    print(\"Total Distance of all routes: {}m\".format(total_distance))\n",
    "    # Assign the values from new_vehicle_assignment to dataset['NEW RT']\n",
    "    # Convert new_vehicle_assignment dictionary to a pandas Series first, then add 1 to align with # At the end of your function, after completing assignments for all vehicles\n",
    "    dataset[\"NEW RT\"] = pd.Series(new_vehicle_assignment).map(\n",
    "        lambda x: x + 1\n",
    "    )  # Convert and correct route indices as needed\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def assign_new_rts(data, manager, routing, solution, dataset):\n",
    "    new_vehicle_assignment = {}\n",
    "    new_stop_assignment = {}\n",
    "    new_dist_assignment = {}\n",
    "\n",
    "    for vehicle_id in range(data[\"num_vehicles\"]):\n",
    "        # print(f\"Vehicle {vehicle_id + 1}:\")\n",
    "        index = solution.Value(routing.NextVar(routing.Start(vehicle_id)))\n",
    "        stop = 1\n",
    "        # Start index for this vehicle\n",
    "        previous_index = routing.Start(vehicle_id)\n",
    "\n",
    "        while not routing.IsEnd(index):\n",
    "            node_index = manager.IndexToNode(index)\n",
    "\n",
    "            # Assignments for current stop\n",
    "            new_vehicle_assignment[node_index] = vehicle_id\n",
    "            new_stop_assignment[node_index] = int(stop)\n",
    "            # print(f\"Stop {stop}: {dataset.iloc[node_index]['CUST NAME']}\")\n",
    "            # print(f\"Index: {index}, Node Index: {node_index}, Stop: {stop}, Previous Index: {previous_index}\")\n",
    "\n",
    "            # Update distance if not at the start node\n",
    "            if not routing.IsStart(previous_index):\n",
    "                # Ensure the previous index and current index are within bounds of the distance matrix\n",
    "                if previous_index < len(data[\"distance_matrix\"]) and index < len(data[\"distance_matrix\"]):\n",
    "                    new_dist_value = data[\"distance_matrix\"][previous_index][index]\n",
    "                    new_dist_assignment[node_index] = new_dist_value\n",
    "\n",
    "            # Update for the next iteration\n",
    "            previous_index = index\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            stop += 1\n",
    "\n",
    "        # Handle the final connection back to the depot, if necessary\n",
    "        if not routing.IsStart(previous_index) and routing.IsEnd(index):\n",
    "            # End node reached, assign default values or perform necessary finalization\n",
    "            # Using previous_index as end node\n",
    "            end_node_index = manager.IndexToNode(previous_index)\n",
    "            new_vehicle_assignment[end_node_index] = vehicle_id\n",
    "            new_stop_assignment[end_node_index] = stop\n",
    "            # No distance back to depot or handle appropriately\n",
    "            new_dist_assignment[end_node_index] = 0\n",
    "\n",
    "    # Update the dataset based on assignments\n",
    "    dataset[\"NEW RT\"] = pd.Series(new_vehicle_assignment).map(lambda x: x + 1)\n",
    "    dataset[\"NEW DAY\"] = pd.Series(new_vehicle_assignment).map(lambda x: x + 1)\n",
    "    dataset[\"NEW DIST\"] = pd.Series(new_dist_assignment)\n",
    "    dataset[\"NEW STOP\"] = pd.Series(new_stop_assignment)\n",
    "    dataset[\"NEW DIST\"] = dataset[\"NEW DIST\"].fillna(0)\n",
    "    dataset[\"NEW STOP\"] = dataset[\"NEW STOP\"].fillna(0)\n",
    "    dataset[\"NEW STOP\"] = dataset[\"NEW STOP\"].astype(int)\n",
    "    dataset[\"NEW DAY\"] = dataset[\"NEW DAY\"].fillna(0)\n",
    "    dataset[\"NEW DAY\"] = dataset[\"NEW DAY\"].astype(int)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_map_with_accessible_tooltips(dataset):\n",
    "    dataset = dataset.sort_values([\"NEW RT\", \"NEW DAY\"])\n",
    "\n",
    "    m = folium.Map(location=[39.7, -76.5], zoom_start=9, max_zoom=20)\n",
    "\n",
    "    vehicles = dataset[\"NEW RT\"].unique()\n",
    "    colors = list(mcolors.CSS4_COLORS.keys())[13: 13 + len(vehicles)]\n",
    "\n",
    "    color_map = dict(zip(vehicles, colors))\n",
    "\n",
    "    for vehicle in vehicles:\n",
    "        vehicle_data = dataset[dataset[\"NEW RT\"] == vehicle]\n",
    "        locations = vehicle_data[[\"Latitude\", \"Longitude\"]].values\n",
    "\n",
    "        route_group = FeatureGroup(name=f\"Route {vehicle}\")\n",
    "\n",
    "        # Group data by route-day to compute hulls separately\n",
    "        for (route, day), day_data in vehicle_data.groupby([\"NEW RT\", \"NEW DAY\"]):\n",
    "            locations = day_data[[\"Latitude\", \"Longitude\"]].values\n",
    "            # print(f\"Locations for route {route} day {day}:\\n{locations}\")\n",
    "            if len(locations) > 2:\n",
    "                try:\n",
    "\n",
    "                    hull = ConvexHull(locations, qhull_options=\"QJ\")\n",
    "                    hull_points = locations[hull.vertices]\n",
    "                    # print(f\"Hull points for route {route} day {day}:\\n{hull_points}\")\n",
    "                    # input(\"Press Enter to continue...\")\n",
    "                    folium.Polygon(\n",
    "                        locations=hull_points.tolist(),\n",
    "                        color=color_map[vehicle],\n",
    "                        fill=True,\n",
    "                        fill_color=color_map[vehicle],\n",
    "                        fill_opacity=0.3,\n",
    "                        weight=2,\n",
    "                    ).add_to(route_group)\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Could not create a convex hull for vehicle {vehicle}: {e}\")\n",
    "\n",
    "                # Markers: (Same as before)\n",
    "                for index, row in day_data.iterrows():\n",
    "                    tooltip_html = f\"\"\"\n",
    "                    Route: {vehicle}, Day: {row['NEW DAY']}<br>\n",
    "                    Customer: {row['CUST NAME']}<br>\n",
    "                    Address: {row['ADDRESS']}<br>\n",
    "                    Sales: ${row['Machine']:,.2f}<br>\n",
    "                    Companion: ${row['Companion']:,.2f}\n",
    "                    \"\"\"\n",
    "                    folium.CircleMarker(\n",
    "                        location=[row[\"Latitude\"], row[\"Longitude\"]],\n",
    "                        tooltip=tooltip_html,\n",
    "                        color=color_map[vehicle],\n",
    "                        fill=True,\n",
    "                        fill_color=color_map[vehicle],\n",
    "                        radius=5,\n",
    "                    ).add_to(route_group)\n",
    "\n",
    "        route_group.add_to(m)\n",
    "\n",
    "    Fullscreen().add_to(m)\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # create map file name with time and date\n",
    "    now = datetime.datetime.now()\n",
    "    map_file = f\"./data/maps/nmap_with_accessible_tooltips {now.day}-{now.month}-{now.year} {now.hour}{now.minute}.html\"\n",
    "    m.save(map_file)\n",
    "    map_file = f\"./data/nmaaap_with_accessible_tooltips.html\"\n",
    "    m.save(map_file)\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "    # display(m)\n",
    "    return m\n",
    "\n",
    "\n",
    "def swap_route_days(dataset, rt1, rt2, day1, day2):\n",
    "    \"\"\"\n",
    "    Swap all stops between two route-day combinations within the dataset, including changing their route numbers.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): The dataset containing route information.\n",
    "        rt1, rt2 (int): The route numbers involved in the swap.\n",
    "        day1, day2 (int): The day numbers to be swapped between the routes.\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    updated_dataset = dataset.copy()\n",
    "\n",
    "    # Identify the index for each route-day combination\n",
    "    index_rt_day_1 = dataset[(dataset['NEW RT'] == rt1) & (\n",
    "        dataset['NEW DAY'] == day1)].index\n",
    "    index_rt_day_2 = dataset[(dataset['NEW RT'] == rt2) & (\n",
    "        dataset['NEW DAY'] == day2)].index\n",
    "\n",
    "    # Check if we have records to swap to avoid errors\n",
    "    if not index_rt_day_1.empty and not index_rt_day_2.empty:\n",
    "        # Swap the route numbers and days between the selected route-day combinations\n",
    "        updated_dataset.loc[index_rt_day_1, 'NEW RT'] = rt2\n",
    "        updated_dataset.loc[index_rt_day_2, 'NEW RT'] = rt1\n",
    "        updated_dataset.loc[index_rt_day_1, 'NEW DAY'] = day2\n",
    "        updated_dataset.loc[index_rt_day_2, 'NEW DAY'] = day1\n",
    "    else:\n",
    "        print(\"One of the routes/days to be swapped does not exist in the dataset.\")\n",
    "\n",
    "    return updated_dataset\n",
    "\n",
    "\n",
    "def move_route_stops(dataset, from_rt, from_day, to_rt, to_day, list_of_stops):\n",
    "    \"\"\"\n",
    "    Move specified stops from one route-day combination to another, \n",
    "    appending them to the end of the new route-day.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): The dataset containing route information.\n",
    "        from_rt, to_rt (int): The original and destination route numbers.\n",
    "        from_day, to_day (int): The original and destination day numbers.\n",
    "        list_of_stops (list): List of stops to be moved.\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    updated_dataset = dataset.copy()\n",
    "\n",
    "    # Filter for the stops to be moved\n",
    "    stops_to_move = updated_dataset[(updated_dataset['NEW RT'] == from_rt) &\n",
    "                                    (updated_dataset['NEW DAY'] == from_day) &\n",
    "                                    (updated_dataset['STOP'].isin(list_of_stops))]\n",
    "\n",
    "    # Check if there are stops to move\n",
    "    if not stops_to_move.empty:\n",
    "        # Update 'NEW RT' and 'NEW DAY' for these stops\n",
    "        updated_dataset.loc[stops_to_move.index, 'NEW RT'] = to_rt\n",
    "        updated_dataset.loc[stops_to_move.index, 'NEW DAY'] = to_day\n",
    "\n",
    "        # Assuming 'NEW STOP' indicates the ordering within the day\n",
    "        # Find the max 'NEW STOP' in the destination route-day to append these stops at the end\n",
    "        max_stop_in_destination = updated_dataset[(updated_dataset['NEW RT'] == to_rt) &\n",
    "                                                  (updated_dataset['NEW DAY'] == to_day)]['NEW STOP'].max()\n",
    "\n",
    "        # Update 'NEW STOP' for the moved stops, appending them to the end\n",
    "        # Start from 1 to add to the max\n",
    "        for i, stop in enumerate(sorted(list_of_stops), start=1):\n",
    "            updated_dataset.loc[(updated_dataset['STOP'] == stop) &\n",
    "                                (updated_dataset['NEW RT'] == to_rt) &\n",
    "                                (updated_dataset['NEW DAY'] == to_day), 'NEW STOP'] = max_stop_in_destination + i\n",
    "    else:\n",
    "        print(\"No matching stops found to move.\")\n",
    "\n",
    "    return updated_dataset\n",
    "\n",
    "\n",
    "def haversine_vectorized(latitudes, longitudes, depot_index=0, unit=\"kilometers\"):\n",
    "    \"\"\"\n",
    "    Vectorized version of the Haversine formula to calculate the pairwise distance matrix between two sets of geographical points.\n",
    "    Additionally incorporates custom logic to simulate the 'distance_callback' adjustments.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat_rad = np.radians(latitudes)\n",
    "    lon_rad = np.radians(longitudes)\n",
    "\n",
    "    # Expand lat_rad and lon_rad into 2D arrays for broadcasting\n",
    "    lat_rad_matrix = np.expand_dims(lat_rad, axis=0)\n",
    "    lon_rad_matrix = np.expand_dims(lon_rad, axis=0)\n",
    "\n",
    "    # Compute pairwise differences\n",
    "    dlat = lat_rad_matrix - lat_rad_matrix.T\n",
    "    dlon = lon_rad_matrix - lon_rad_matrix.T\n",
    "\n",
    "    # Haversine formula\n",
    "    a = (\n",
    "        np.sin(dlat / 2.0) ** 2\n",
    "        + np.cos(lat_rad_matrix) * np.cos(lat_rad_matrix.T) *\n",
    "        np.sin(dlon / 2.0) ** 2\n",
    "    )\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance_matrix = EARTH_RADIUS_KM * c  # Distance in kilometers\n",
    "\n",
    "    # Convert distance according to the unit\n",
    "    conversion_factors = {\n",
    "        \"miles\": 0.621371,\n",
    "        \"kilometers\": 1,\n",
    "        \"nautical miles\": 0.539957,\n",
    "    }\n",
    "    if unit in conversion_factors:\n",
    "        distance_matrix *= conversion_factors[unit]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid unit\")\n",
    "\n",
    "    # Custom logic adjustment based on distance_callback rules\n",
    "    # Adding additional cost based on depot distances\n",
    "    for from_node in range(len(latitudes)):\n",
    "        from_depot_cost = distance_matrix[depot_index][from_node]\n",
    "        for to_node in range(len(latitudes)):\n",
    "            to_depot_cost = distance_matrix[depot_index][to_node]\n",
    "            if to_depot_cost > from_depot_cost:\n",
    "                distance_matrix[from_node][to_node] += 11\n",
    "    # set the first row and column to 0\n",
    "    distance_matrix[0] = 0\n",
    "    distance_matrix[:, 0] = 0\n",
    "    \n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "def create_distance_matrix(dataset):\n",
    "    # Example usage\n",
    "    latitudes = dataset[\"Latitude\"].to_numpy()\n",
    "    longitudes = dataset[\"Longitude\"].to_numpy()\n",
    "    # time the function\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    # print(\"Creating distance matrix...\")\n",
    "\n",
    "    distance_matrix = haversine_vectorized(\n",
    "        latitudes, longitudes, unit=\"kilometers\")\n",
    "    # calculate the time taken\n",
    "    end = time.time()\n",
    "    # print(f\"Time taken: {end - start} seconds\")\n",
    "    # Convert the distance matrix to a DataFrame if needed\n",
    "    distance_matrix = pd.DataFrame(\n",
    "        distance_matrix, index=dataset.index, columns=dataset.index\n",
    "    )\n",
    "    distance_matrix *= 10\n",
    "    distance_matrix += 0.9999\n",
    "    # multiply all values of distance_matrix by 2 and convert to int\n",
    "    distance_matrix = distance_matrix.astype(int)\n",
    "\n",
    "    # Create a boolean mask where all diagonal elements are False and others are True\n",
    "    mask = ~np.eye(distance_matrix.shape[0], dtype=bool)\n",
    "\n",
    "    # Add 1 to all elements that are not on the diagonal (mask is True) and are currently 0\n",
    "    distance_matrix = distance_matrix.mask(\n",
    "        mask & (distance_matrix == 0), distance_matrix + 1\n",
    "    )\n",
    "    distance_matrix = distance_matrix.astype(int)\n",
    "\n",
    "    distance_matrix.to_csv(\"distance_matrix.csv\", index=False)\n",
    "    # print(\"Distance matrix created.\")\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "def find_unique_name_indexes(data_frame, custom_names):\n",
    "    \"\"\"\n",
    "    Finds the indexes of unique custom names in a DataFrame's 'CUST NAME' column.\n",
    "\n",
    "    Args:\n",
    "        data_frame: A Pandas DataFrame.\n",
    "        custom_names: A list of unique custom names to search for.\n",
    "\n",
    "    Returns:\n",
    "        A list of the indexes of the custom names in the DataFrame. If a name\n",
    "        is not found, its corresponding index in the result list will be None.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for name in custom_names:\n",
    "        mask = data_frame['CUST NAME'] == name\n",
    "        index = data_frame.loc[mask].index.tolist()  # Get index as a list\n",
    "        result.append(index[0] if index else None)  # First index or None\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def best_tracker_with_params(solution, filename, search_parameters):\n",
    "    current_obj = solution.ObjectiveValue()\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                prev_obj = int(lines[-1])\n",
    "                best_obj = int(lines[-2])\n",
    "            else:\n",
    "                prev_obj = current_obj\n",
    "                best_obj = current_obj\n",
    "    except FileNotFoundError:\n",
    "        prev_obj = current_obj\n",
    "        best_obj = current_obj\n",
    "\n",
    "    # Create a table object for best tracker\n",
    "    best_tracker_table = Table(title=\"Progress Tracker\", show_lines=True)\n",
    "    best_tracker_table.add_column(\"Solution\", style=\"magenta\")\n",
    "    best_tracker_table.add_column(\"Value\", style=\"cyan\")\n",
    "    best_tracker_table.add_row(\"[b]Current\", \"[b]{:,}\".format(current_obj))\n",
    "\n",
    "    best_tracker_table.add_row(\"Best\", \"{:,}\".format(best_obj))\n",
    "    best_tracker_table.add_row(\"Last Run\", \"{:,}\".format(prev_obj))\n",
    "\n",
    "    if best_obj > current_obj:\n",
    "        improvement = best_obj - current_obj\n",
    "        best_obj = current_obj\n",
    "        best_tracker_table.add_row(\n",
    "            \"[b]WINNER![/b]\", \"[b]{:,}\".format(best_obj))\n",
    "        best_tracker_table.add_row(\n",
    "            \"[b]IMPROVEMENT AMOUNT\", \"[b]{:,}\".format(improvement))\n",
    "    else:\n",
    "        if prev_obj > current_obj:\n",
    "            improvement = prev_obj - current_obj\n",
    "            best_tracker_table.add_row(\n",
    "                \"[b]IMPROVEMENT AMOUNT\", \"[b]{:,}\".format(improvement))\n",
    "        else:\n",
    "            deterioration = current_obj - prev_obj\n",
    "            best_tracker_table.add_row(\n",
    "                \"[b]WORSE BY\", \"[b]{:,}\".format(deterioration))\n",
    "    best_tracker_table.add_row(\"Explore Coefficient\", \"{:,}\".format(\n",
    "        search_parameters.multi_armed_bandit_compound_operator_exploration_coefficient))\n",
    "    best_tracker_table.add_row(\"Memory Coefficient\", \"{:1f}\".format(\n",
    "        search_parameters.multi_armed_bandit_compound_operator_memory_coefficient))\n",
    "    best_tracker_table.add_row(\"Solve Time (seconds)\", str(\n",
    "        search_parameters.time_limit.seconds))\n",
    "    # After processing, update the file with the current and new best solutions\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"{best_obj}\\n{current_obj}\\n\")\n",
    "    return best_tracker_table\n",
    "\n",
    "\n",
    "def process_solution(data, manager, routing, solution, dataset):\n",
    "    total_distance = 0\n",
    "    total_load = 0\n",
    "    total_load2 = 0\n",
    "    route_details = []\n",
    "    new_vehicle_assignment = {}  # This will map node index to vehicle ID\n",
    "\n",
    "    for vehicle_id in range(data[\"num_vehicles\"]):\n",
    "        index = routing.Start(vehicle_id)\n",
    "        route_distance = 0\n",
    "        route_load = 0\n",
    "        route_load2 = 0\n",
    "        route_count = 0\n",
    "        while not routing.IsEnd(index):\n",
    "            node_index = manager.IndexToNode(index)\n",
    "            route_load += data[\"demands\"][node_index]\n",
    "            route_load2 += data[\"demands2\"][node_index]\n",
    "            previous_index = index\n",
    "            index = solution.Value(routing.NextVar(index))\n",
    "            route_distance += routing.GetArcCostForVehicle(\n",
    "                previous_index, index, vehicle_id)\n",
    "            route_count += 1\n",
    "            # Assign the vehicle to the nodes it visits\n",
    "            # +1 for 1-indexed vehicle IDs\n",
    "            new_vehicle_assignment[node_index] = vehicle_id + 1\n",
    "\n",
    "        # Add the end node for each vehicle\n",
    "        new_vehicle_assignment[manager.IndexToNode(\n",
    "            index)] = vehicle_id + 1  # Assign end node\n",
    "\n",
    "        total_distance += route_distance\n",
    "        total_load += route_load\n",
    "        total_load2 += route_load2\n",
    "\n",
    "        # Collect route details for return\n",
    "        route_details.append({\n",
    "            'vehicle_id': vehicle_id + 1,\n",
    "            'stops': route_count,\n",
    "            'load': route_load,\n",
    "            'load2': route_load2,\n",
    "            'distance': route_distance\n",
    "        })\n",
    "\n",
    "    # Apply new vehicle assignments to dataset\n",
    "    # Ensure every index/node is in new_vehicle_assignment, then update 'NEW RT'\n",
    "    dataset['NEW RT'] = dataset.index.map(\n",
    "        lambda idx: new_vehicle_assignment.get(idx, np.nan))\n",
    "    dataset['NEW RT'] = dataset['NEW RT'].fillna(\n",
    "        method='ffill').fillna(method='bfill')\n",
    "\n",
    "    # Compile overall results\n",
    "    results = {\n",
    "        'dataset': dataset,\n",
    "        'total_distance': total_distance,\n",
    "        'total_load': total_load,\n",
    "        'total_load2': total_load2,\n",
    "        'route_details': route_details\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def combined_tracker_and_statistics(solution, filename, dataset, data, manager, routing, search_parameters):\n",
    "    # Assuming you have other code here for setup...\n",
    "\n",
    "    # Now call your updated best tracker with parameters\n",
    "\n",
    "    # Assuming you have more code here for the rest of the function...\n",
    "    \"\"\"\n",
    "    Combines tracking of solution quality, route statistics, and solution specifics \n",
    "    into a comprehensive overview using Rich tables for readability and user-friendliness.\n",
    "\n",
    "    Args:\n",
    "        solution: The routing solution object.\n",
    "        filename: Name of the file used to track the best and previous objectives.\n",
    "        dataset: The dataset containing route information.\n",
    "        data: The data dictionary used in the routing problem.\n",
    "        manager: The index manager for the routing problem.\n",
    "        routing: The routing model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    # returns a table of the best tracker\n",
    "    # sleep(1)\n",
    "    best_tracker_table = best_tracker_with_params(\n",
    "        solution, filename, search_parameters)\n",
    "\n",
    "    # Generate route statistics and integrate them into the dataset\n",
    "    route_data = process_solution(data, manager, routing, solution, dataset)\n",
    "\n",
    "    # Prepare the tables for display\n",
    "    stats_table = Table(title=\"Route Statistics\", show_lines=True)\n",
    "    # stats_table.add_column(\"Route ID\", style=\"magenta\")\n",
    "    stats_table.add_column(\"Total BIG 5 AVG\", style=\"cyan\", max_width=10)\n",
    "    stats_table.add_column(\"Total COMP AVG\", style=\"green\", max_width=7)\n",
    "    stats_table.add_column(\"Number of Days\", style=\"cyan\", max_width=6)\n",
    "    stats_table.add_column(\"Number of Stops\", style=\"green\", max_width=6)\n",
    "    # stats_table.add_column(\"Average Stops/Day\", style=\"red\",max_width=7)\n",
    "    stats_table.add_column(\"Ttl Distance\", style=\"cyan\", max_width=8)\n",
    "    # stats_table.add_column(\"BIG 5/Stop\", style=\"blue\",max_width=7)\n",
    "    stats_table.add_column(\"COMP / Stop\", style=\"green\", max_width=7)\n",
    "\n",
    "    # Populate route statistics table\n",
    "    route_stats = generate_route_statistics(dataset)\n",
    "    for _, row in route_stats.iterrows():\n",
    "        stats_table.add_row(\n",
    "            # f\"{int(row['NEW RT'])}\",\n",
    "            f\"${int(row['total_BIG_5_AVG']):,}\",\n",
    "            f\"${int(row['total_COMP_AVG']):,}\",\n",
    "            f\"{int(row['number_of_NEW_DAY'])}\",\n",
    "            f\"{int(row['number_of_NEW_STOP'])}\",\n",
    "            # f\"{row['average_stops_per_day']:.1f}\",\n",
    "            f\"{int(row['average_NEW_DIST']):,}\",\n",
    "            # f\"${row['total_BIG_5_AVG']/row['number_of_NEW_STOP']:,.2f}\",\n",
    "            f\"${int(row['total_COMP_AVG']/row['number_of_NEW_STOP'])}\"\n",
    "        )\n",
    "    console = Console()\n",
    "\n",
    "    panel1 = Panel(best_tracker_table, title=\"Best Tracker\")\n",
    "    panel2 = Panel(stats_table)\n",
    "    tables_side_by_side = Columns([best_tracker_table, stats_table])\n",
    "    # Print the tables\n",
    "    print(tables_side_by_side)\n",
    "    return dataset  # Return the modified dataset for potential further use\n",
    "\n",
    "\n",
    "def generate_route_statistics(dataset):\n",
    "    # Group the data by 'NEW RT' and calculate the required statistics\n",
    "    route_stats = (\n",
    "        dataset.groupby(\"NEW RT\")\n",
    "        .agg(\n",
    "            total_BIG_5_AVG=(\"BIG 5 AVG\", \"sum\"),\n",
    "            total_COMP_AVG=(\"COMP AVG\", \"sum\"),\n",
    "            number_of_NEW_DAY=(\"NEW DAY\", \"nunique\"),\n",
    "            number_of_NEW_STOP=(\"NEW STOP\", \"count\"),\n",
    "            average_NEW_DIST=(\"NEW DIST\", \"sum\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Calculate Average Stops per day for each route\n",
    "    route_stats[\"average_stops_per_day\"] = (\n",
    "        route_stats[\"number_of_NEW_STOP\"] / route_stats[\"number_of_NEW_DAY\"]\n",
    "    )\n",
    "    return route_stats\n",
    "\n",
    "\n",
    "def add_lat_long_to_df(dataset):\n",
    "    api_key = \"8dad1cc087374a6ba988e6371e45911b\"\n",
    "    # Update Coordinates\n",
    "    # Check for existing coordinates file\n",
    "    if os.path.exists(\"./coordinates.csv\"):\n",
    "        # print(\"Coordinates file exists.\")\n",
    "        coordinates_df = (\n",
    "            pd.read_csv(\"./coordinates.csv\")\n",
    "            .drop_duplicates(subset=\"CUST\")\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        coordinates_df[\"CUST\"] = coordinates_df[\"CUST\"].astype(\n",
    "            str\n",
    "        )  # Convert the 'CUST' column to string\n",
    "        print(coordinates_df[\"CUST\"])\n",
    "    else:\n",
    "        print(\"Creating new coordinates DataFrame...\")\n",
    "        coordinates_df = pd.DataFrame(\n",
    "            columns=[\"CUST\", \"Latitude\", \"Longitude\"])\n",
    "    print(\"Updating coordinates...\")\n",
    "    # Iterate through the dataset\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        cust = str(dataset.at[i, \"CUST\"])  # Ensure CUST is treated as a string\n",
    "\n",
    "        # Check if the customer is already in the coordinates DataFrame\n",
    "        if cust in coordinates_df[\"CUST\"].values:\n",
    "            # print(f\"Using existing coordinates for {dataset.at[i, 'CUST NAME']}\")\n",
    "            row = coordinates_df[coordinates_df[\"CUST\"] == cust]\n",
    "            dataset.at[i, \"Latitude\"] = row[\"Latitude\"].values[0]\n",
    "            dataset.at[i, \"Longitude\"] = row[\"Longitude\"].values[0]\n",
    "        else:\n",
    "            # Construct the address\n",
    "            print(f\"I = {i}    Name: {dataset.at['CUST NAME']}\")\n",
    "            name = dataset.at[i, \"CUST NAME\"]\n",
    "            house_number = dataset.at[i, \"HOUSE NUMBER\"]\n",
    "            address = dataset.at[i, \"ADDRESS\"]\n",
    "            city = dataset.at[i, \"CITY\"]\n",
    "            short_zip = str(dataset.at[i, \"ZIP\"])[:5]\n",
    "            print(\n",
    "                f\"Name: {name}, House Number: {house_number}, Address: {address}, City: {city}, ZIP: {short_zip}\")\n",
    "            print(f\"Quote Plus Name: {quote_plus(name)}\")\n",
    "            print(f\"Quote Plus House Number: {quote_plus(house_number)}\")\n",
    "            print(f\"Quote Plus Address: {quote_plus(address)}\")\n",
    "            print(f\"Quote Plus City: {quote_plus(city)}\")\n",
    "            print(f\"Quote Plus ZIP: {quote_plus(short_zip)}\")\n",
    "\n",
    "            # Encode components for URL\n",
    "            url = f\"https://api.geoapify.com/v1/geocode/search?name={quote_plus(name)}&housenumber={quote_plus(house_number)}&street={quote_plus(address)}&city={quote_plus(city)}&postcode={short_zip}&format=json&apiKey={api_key}\"\n",
    "            print(url)\n",
    "            # Make the request\n",
    "            resp = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                resp_query = resp.json()\n",
    "                if resp_query[\"results\"]:\n",
    "                    lat, lon = (\n",
    "                        resp_query[\"results\"][0][\"lat\"],\n",
    "                        resp_query[\"results\"][0][\"lon\"],\n",
    "                    )\n",
    "                    dataset.at[i, \"Latitude\"] = lat\n",
    "                    dataset.at[i, \"Longitude\"] = lon\n",
    "                    # Append new coordinates to coordinates_df and the CSV file\n",
    "                    new_row = pd.DataFrame(\n",
    "                        [[cust, lat, lon]], columns=[\n",
    "                            \"CUST\", \"Latitude\", \"Longitude\"]\n",
    "                    )\n",
    "                    coordinates_df = pd.concat(\n",
    "                        [coordinates_df, new_row], ignore_index=True)\n",
    "                    new_row.to_csv(\"coordinates.csv\", mode=\"a\",\n",
    "                                   header=False, index=False)\n",
    "                    print(f\"Added coordinates for {name}\")\n",
    "                else:\n",
    "                    print(f\"No results found for {name}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Failed to get coordinates for {name}: HTTP {resp.status_code}\")\n",
    "    dataset.to_csv(\"updated_dataset.csv\", index=False)\n",
    "    # Save the updated coordinates DataFrame\n",
    "    coordinates_df.to_csv(\"coordinates.csv\", index=False, mode=\"w\")\n",
    "    return dataset\n",
    "\n",
    "    # save dataset to csv\n",
    "\n",
    "\n",
    "# Constants\n",
    "EARTH_RADIUS_KM = 6371  # Earth's radius in kilometers\n",
    "R = 6371.0\n",
    "\n",
    "# Setup Dataset\n",
    "file_path = \"./data/uploads/\"\n",
    "file_name = \"P4reroute.xlsx\"\n",
    "def load_depot():\n",
    "    depot = pd.read_csv(filepath_or_buffer=\"depot.csv\")\n",
    "    dataset = pd.concat([depot, dataset]).reset_index(drop=True)\n",
    "\n",
    "# load reroute.xls into dataset, set the column headers to the first row of the dataset and drop the second row\n",
    "dataset = pd.read_excel(file_path + file_name)\n",
    "dataset = dataset.iloc[1:, 0:]\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "# if dataset does not have a column named 'Latitude' then create it\n",
    "if \"Latitude\" not in dataset.columns:\n",
    "    dataset[\"Latitude\"] = np.nan\n",
    "# if dataset does not have a column named 'Longitude' then create it\n",
    "if \"Longitude\" not in dataset.columns:\n",
    "    dataset[\"Longitude\"] = np.nan\n",
    "\n",
    "try:\n",
    "    dataset[\"Machine\"] = dataset[\"BIG 5 AVG\"]\n",
    "    dataset[\"Companion\"] = dataset[\"COMP AVG\"]\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Assuming 'dataset' is your DataFrame\n",
    "# Use regular expressions to remove any non-numeric characters from the 'NEW DAY' column\n",
    "dataset['DAY'] = dataset['DAY'].replace(\n",
    "    to_replace=[r'[^\\d]+'], value='', regex=True)\n",
    "\n",
    "# Convert the cleaned 'NEW DAY' values from strings to integers\n",
    "# Note: This will convert empty strings to NaN, which you might need to handle based on your use case\n",
    "dataset['DAY'] = pd.to_numeric(\n",
    "    dataset['DAY'], errors='coerce').fillna(0).astype(int)\n",
    "dataset[\"RT\"] = dataset[\"RT\"].astype(int)\n",
    "# split the 'ADDRESS' column by comma and keep only the first part\n",
    "dataset[\"ADDRESS\"] = dataset[\"ADDRESS\"].str.split(\",\").str[0]\n",
    "\n",
    "dataset = add_lat_long_to_df(dataset)\n",
    "\n",
    "\n",
    "distance_matrix = create_distance_matrix(dataset)\n",
    "latlong_df = dataset[[\"Latitude\", \"Longitude\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset['NEW DAY'] = dataset['NEW DAY'].replace(\n",
    "    to_replace=[r'[^\\d]+'], value='', regex=True)\n",
    "\n",
    "# Convert the cleaned 'NEW DAY' values from strings to integers\n",
    "# Note: This will convert empty strings to NaN, which you might need to handle based on your use case\n",
    "dataset['NEW DAY'] = pd.to_numeric(\n",
    "    dataset['NEW DAY'], errors='coerce').fillna(0).astype(int)\n",
    "dataset[\"NEW RT\"] = dataset[\"NEW RT\"].fillna(0).astype(int)\n",
    "dataset['NEW STOP'] = dataset['NEW STOP'].fillna(0).astype(int)\n",
    "# Create a 'Stop ID' column using the row index\n",
    "dataset['Stop ID'] = dataset.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_search_parameters(\n",
    "    search_parameters,\n",
    "    solve_minutes=1,\n",
    "    explore_coefficient=99999999,\n",
    "    memory_coefficient=0.09,\n",
    "    log_search=False,\n",
    "    seconds_per_iter=10,\n",
    "    solution_limit=999999,\n",
    "):\n",
    "    \"\"\"\n",
    "    Set the search parameters for the routing problem.\n",
    "\n",
    "    Args:\n",
    "        search_parameters: The search parameters object.\n",
    "        solve_minutes: The number of minutes to solve the problem (default: 1).\n",
    "        explore_coefficient: The exploration coefficient for the multi-armed bandit compound operator (default: 99999999).\n",
    "        memory_coefficient: The memory coefficient for the multi-armed bandit compound operator (default: 0.09).\n",
    "\n",
    "    Returns:\n",
    "        The updated search parameters object.\n",
    "    \"\"\"\n",
    "    F = pywrapcp.BOOL_FALSE\n",
    "    T = pywrapcp.BOOL_TRUE\n",
    "\n",
    "    search_parameters.local_search_operators.use_cross = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_relocate = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_exchange = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_cross_exchange = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_tsp_opt = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_or_opt = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_lin_kernighan = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_two_opt = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_relocate_neighbors = pywrapcp.BOOL_TRUE\n",
    "\n",
    "    search_parameters.local_search_operators.use_exchange_subtrip = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_exchange_pair = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_make_inactive = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_light_relocate_pair = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_relocate_subtrip = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_relocate_and_make_active = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_extended_swap_active = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_node_pair_swap_active = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_swap_active = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_make_chain_inactive = pywrapcp.BOOL_TRUE\n",
    "\n",
    "    search_parameters.local_search_operators.use_relocate_expensive_chain = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_full_path_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_inactive_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_tsp_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_path_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_full_path_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_global_cheapest_insertion_close_nodes_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_local_cheapest_insertion_close_nodes_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_global_cheapest_insertion_expensive_chain_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_local_cheapest_insertion_expensive_chain_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_global_cheapest_insertion_path_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.local_search_operators.use_local_cheapest_insertion_path_lns = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.use_multi_armed_bandit_concatenate_operators = pywrapcp.BOOL_TRUE\n",
    "\n",
    "    search_parameters.multi_armed_bandit_compound_operator_memory_coefficient = (\n",
    "        memory_coefficient)\n",
    "    search_parameters.multi_armed_bandit_compound_operator_exploration_coefficient = (\n",
    "        explore_coefficient)\n",
    "\n",
    "    search_parameters.relocate_expensive_chain_num_arcs_to_consider = 90\n",
    "    search_parameters.heuristic_expensive_chain_lns_num_arcs_to_consider = 99\n",
    "    search_parameters.heuristic_close_nodes_lns_num_nodes = 90\n",
    "    search_parameters.use_depth_first_search = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.solution_limit = solution_limit\n",
    "    # search_parameters.time_limit.seconds = int(\n",
    "    #     solve_minutes * seconds_per_iter)  # compute_time.value\n",
    "    # search_parameters.lns_time_limit.nanos = 1\n",
    "    # search_parameters.number_of_solutions_to_collect = 3\n",
    "\n",
    "    search_parameters.lns_time_limit.seconds = 31\n",
    "    search_parameters.use_cp_sat = pywrapcp.BOOL_TRUE\n",
    "    search_parameters.fallback_to_cp_sat_size_threshold = 99\n",
    "    search_parameters.cheapest_insertion_first_solution_min_neighbors = 9\n",
    "    search_parameters.cheapest_insertion_first_solution_neighbors_ratio = 0.9\n",
    "    # search_parameters.secondary_ls_time_limit_ratio = 0.04\n",
    "    log_search = True\n",
    "    search_parameters.log_search = log_search\n",
    "    search_parameters.use_full_propagation = False\n",
    "    # search_parameters.log_cost_scaling_factor = 0.1\n",
    "    return search_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prime the Routing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_model(\n",
    "    dataset,\n",
    "    distance_matrix,\n",
    "    vehicle_capacities=[],\n",
    "    vehicle_capacities2=[],\n",
    "    vehicle_capacities3=[],\n",
    "    starts=[],\n",
    "    ends=[]\n",
    "):\n",
    "    max_stops_per_day = 9\n",
    "    max_comp_per_day = 1900\n",
    "    max_big5_per_day = 3500\n",
    "    data = {}\n",
    "    demands = dataset[\"Companion\"].tolist()\n",
    "    demands2 = dataset[\"Machine\"].tolist()\n",
    "    data[\"num_vehicles\"] = 50\n",
    "    data[\"distance_matrix\"] = distance_matrix.values.tolist()\n",
    "    data[\"demands\"] = [int(demand) for demand in demands]\n",
    "    data[\"demands2\"] = [int(demand) for demand in demands2]\n",
    "    data[\"demands3\"] = [1] * len(data[\"demands\"])\n",
    "    # data['vehicle_capacities']  = [int(dataset[\"COMP AVG\"].sum() / data['num_vehicles'])] * data['num_vehicles'] # comp target max for each route\n",
    "    # data['vehicle_capacities2'] = [int(dataset[\"BIG 5 AVG\"].sum() / data['num_vehicles'])] * data['num_vehicles'] # big 5 target max for each route\n",
    "    data['vehicle_capacities'] = [max_comp_per_day] * \\\n",
    "        data['num_vehicles']  # comp target max for each route\n",
    "    data['vehicle_capacities2'] = [max_big5_per_day] * \\\n",
    "        data['num_vehicles']  # big 5 target max for each route\n",
    "    # number of stops for each route[int(dataset[\"CUST\"].count() / data['num_vehicles'])]\n",
    "    data['vehicle_capacities3'] = [max_stops_per_day] * data['num_vehicles']\n",
    "    # data[\"vehicle_capacities\"] = [int(capacity)\n",
    "\n",
    "    cap_factor = 3.8\n",
    "    # data['vehicle_capacities'] = [int(15  * capacity) for capacity in data['vehicle_capacities']]\n",
    "    # data['vehicle_capacities2'] = [int(1.2 * capacity) for capacity in data['vehicle_capacities2']]\n",
    "    # data['vehicle_capacities3'] = [int(1.5 * capacity) for capacity in data['vehicle_capacities3']]\n",
    "    data[\"depot\"] = 0\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_for_prime = 22\n",
    "test_runs = 0\n",
    "max_test_runs = 8\n",
    "explore_prime = 0\n",
    "relocate_expensive_arcs = 33\n",
    "heuristic_expensive_arcs = 33\n",
    "heuristic_close_nodes = 33\n",
    "memory_coefficient = 0.77\n",
    "max_distance = 1878\n",
    "solution_limit = 2900\n",
    "final_fig = [None] * solution_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Click to run Solver#############################\n",
    "import random\n",
    "\n",
    "best_obj = max_distance\n",
    "\n",
    "filenames = []\n",
    "\n",
    "\n",
    "data = create_data_model(\n",
    "    dataset,\n",
    "    distance_matrix,\n",
    ")\n",
    "# print(data)\n",
    "manager = pywrapcp.RoutingIndexManager(\n",
    "    len(data[\"distance_matrix\"]), data[\"num_vehicles\"], data[\"depot\"]\n",
    ")\n",
    "\n",
    "routing_parameters = pywrapcp.DefaultRoutingModelParameters()\n",
    "# Create Routing Model.\n",
    "routing = pywrapcp.RoutingModel(manager, routing_parameters)\n",
    "\n",
    "\n",
    "routes = {}\n",
    "for index, row in dataset.iterrows():\n",
    "    if (row[\"NEW RT\"], row[\"NEW DAY\"]) not in routes:\n",
    "        routes[(row[\"NEW RT\"], row[\"NEW DAY\"])] = []\n",
    "    routes[(row[\"NEW RT\"], row[\"NEW DAY\"])].append(index)  # Store the row index\n",
    "\n",
    "\n",
    "# manager = pywrapcp.RoutingIndexManager(len(data[\"distance_matrix\"]), data[\"num_vehicles\"], data[\"starts\"], data[\"ends\"])\n",
    "\n",
    "\n",
    "def create_distance_callback(data, manager, vehicle):\n",
    "    _distances = data[\"distance_matrix\"]\n",
    "    # _starts = data[\"starts\"]\n",
    "    # _ends = data[\"ends\"]\n",
    "    # Capture current value of vehicle\n",
    "\n",
    "    def distance_callback(from_index, to_index, _vehicle=vehicle):\n",
    "        # Convert from routing variable Index to distance matrix NodeIndex.\n",
    "        from_node = manager.IndexToNode(from_index)\n",
    "        to_node = manager.IndexToNode(to_index)\n",
    "        # depot_index = manager.GetDepot()\n",
    "        # depot_node = manager.IndexToNode(depot_index)\n",
    "        # start_node = _starts[_vehicle]\n",
    "        # end_node = _ends[_vehicle]\n",
    "        # _home_cost = _distances[start_node][to_node]\n",
    "        # _end_cost = _distances[to_node][end_node]\n",
    "        to_cost = _distances[from_node][to_node]\n",
    "        # from_depot_cost = _distances[0][from_node]\n",
    "        # to_depot_cost = _distances[0][to_node]\n",
    "        # if (to_node == 242 or to_node == 201 or to_node == 194) and vehicle != 1:\n",
    "        #     to_cost += 4100\n",
    "        # if (from_node == 242 or from_node == 201 or to_node == 194) and vehicle != 1:\n",
    "        #     to_cost += 4100\n",
    "        # off_track_factor = ((_home_cost + _end_cost)/8) ** 2\n",
    "        # _arc_cost = int((to_cost * off_track_factor) / 10000)\n",
    "        # check to see if to or from index is for the depot\n",
    "        # if routing.GetDepot() == from_node or routing.GetDepot() == to_node:\n",
    "        # return 0\n",
    "        # if from_node == manager.GetDepot() or to_node == manager.GetDepot():\n",
    "        #     return 0\n",
    "        # if to_cost > 20:\n",
    "        #     to_cost = int((to_cost /15) ** 2)\n",
    "        # else:\n",
    "        #     to_cost //= 2\n",
    "        # if routing.VehicleRouteConsideredVar(_vehicle) != 1:\n",
    "        # # if routing.ActiveVehicleVar(_vehicle) == 1:\n",
    "        #     to_cost += 350\n",
    "        #     # print(_vehicle)\n",
    "        # if routing.GetDepot() == from_index:\n",
    "        #     to_cost = 1#\n",
    "        # else:\n",
    "        #     to_cost += 350\n",
    "\n",
    "        # if routing.GetDepot() == to_index:\n",
    "        #     # print(from_index)\n",
    "        #     to_cost += 1# random.randint(1, 2)\n",
    "        # # low_penalty = get_num_stops(routing, solution, vehicle)\n",
    "        #     # print(f\"from_node: {from_node} to_node: {to_node} to_index: {to_index} depot: {routing.GetDepot()}\")\n",
    "        # arc_cost =int(((to_cost**2)/100) - ((to_depot_cost + from_depot_cost )/9))\n",
    "        # arc_cost = to_cost\n",
    "        # if to_depot_cost > from_depot_cost:\n",
    "        #     arc_cost = int(arc_cost + 11)\n",
    "\n",
    "        return to_cost\n",
    "\n",
    "    return distance_callback\n",
    "\n",
    "\n",
    "rSolve = routing.solver()\n",
    "\n",
    "for vehicle_id in range(data[\"num_vehicles\"]):\n",
    "    distance_callback = create_distance_callback(data, manager, vehicle_id)\n",
    "    distance_callback_index = routing.RegisterTransitCallback(distance_callback)\n",
    "    routing.SetArcCostEvaluatorOfVehicle(distance_callback_index, vehicle_id)\n",
    "\n",
    "\n",
    "def demand_callback(from_index):\n",
    "    return data[\"demands\"][manager.IndexToNode(from_index)]\n",
    "\n",
    "\n",
    "def demand_callback2(from_index):\n",
    "    return data[\"demands2\"][manager.IndexToNode(from_index)]\n",
    "\n",
    "\n",
    "def demand_callback3(from_index):\n",
    "    return data[\"demands3\"][manager.IndexToNode(from_index)]\n",
    "\n",
    "\n",
    "# Create and register 3 transit callbacks. 1=companion 2=big 5 and 3=1 to count a single stop\n",
    "demand_callback_index = routing.RegisterUnaryTransitCallback(demand_callback)\n",
    "demand_callback_index2 = routing.RegisterUnaryTransitCallback(demand_callback2)\n",
    "demand_callback_index3 = routing.RegisterUnaryTransitCallback(demand_callback3)\n",
    "\n",
    "routing.AddDimension(distance_callback_index, 150, max_distance, True, \"Distance\")\n",
    "routing.AddDimensionWithVehicleCapacity(\n",
    "    demand_callback_index, 100, data[\"vehicle_capacities\"], True, \"Capacity\"\n",
    ")\n",
    "routing.AddDimensionWithVehicleCapacity(\n",
    "    demand_callback_index2, 2000, data[\"vehicle_capacities2\"], True, \"Capacity2\"\n",
    ")\n",
    "routing.AddDimensionWithVehicleCapacity(\n",
    "    demand_callback_index3, 2, data[\"vehicle_capacities3\"], True, \"Capacity3\"\n",
    ")\n",
    "# for vehicle_id in range(data[\"num_vehicles\"]):\n",
    "#     routing.SetVehicleUsedWhenEmpty(True, vehicle_id)\n",
    "##################################################################\n",
    "search_parameters = pywrapcp.DefaultRoutingSearchParameters()\n",
    "\n",
    "search_parameters.first_solution_strategy = (\n",
    "    routing_enums_pb2.FirstSolutionStrategy.LOCAL_CHEAPEST_COST_INSERTION\n",
    ")\n",
    "solve_minutes = 1\n",
    "\n",
    "m = []\n",
    "search_parameters.local_search_metaheuristic = (\n",
    "    routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH\n",
    ")\n",
    "\n",
    "search_parameters.local_search_operators.use_cross = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_relocate = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_exchange = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_cross_exchange = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_tsp_opt = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_or_opt = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_lin_kernighan = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_two_opt = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_relocate_neighbors = pywrapcp.BOOL_TRUE\n",
    "\n",
    "search_parameters.local_search_operators.use_exchange_subtrip = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_exchange_pair = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_make_inactive = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_light_relocate_pair = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_relocate_subtrip = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_relocate_and_make_active = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_extended_swap_active = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_node_pair_swap_active = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_swap_active = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_make_chain_inactive = pywrapcp.BOOL_TRUE\n",
    "\n",
    "search_parameters.local_search_operators.use_relocate_expensive_chain = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_full_path_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_inactive_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_tsp_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_path_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_full_path_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_global_cheapest_insertion_close_nodes_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_local_cheapest_insertion_close_nodes_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_global_cheapest_insertion_expensive_chain_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_local_cheapest_insertion_expensive_chain_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_global_cheapest_insertion_path_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.local_search_operators.use_local_cheapest_insertion_path_lns = pywrapcp.BOOL_TRUE\n",
    "search_parameters.use_multi_armed_bandit_concatenate_operators = pywrapcp.BOOL_TRUE\n",
    "search_parameters.multi_armed_bandit_compound_operator_memory_coefficient = memory_coefficient\n",
    "search_parameters.multi_armed_bandit_compound_operator_exploration_coefficient = explore_prime\n",
    "search_parameters.relocate_expensive_chain_num_arcs_to_consider = relocate_expensive_arcs\n",
    "search_parameters.heuristic_expensive_chain_lns_num_arcs_to_consider = heuristic_expensive_arcs\n",
    "search_parameters.heuristic_close_nodes_lns_num_nodes = heuristic_close_nodes\n",
    "\n",
    "search_parameters.time_limit.seconds = int(\n",
    "    solve_minutes * seconds_for_prime\n",
    ")  # compute_time.value\n",
    "search_parameters.solution_limit = solution_limit\n",
    "search_parameters.lns_time_limit.seconds = 19\n",
    "# search_parameters.secondary_ls_time_limit_ratio = 0.05\n",
    "search_parameters.log_search = True\n",
    "search_parameters.use_full_propagation = False\n",
    "\n",
    "\n",
    "def get_stop_id_from_index(index):\n",
    "    return dataset.loc[index, \"Stop ID\"]\n",
    "\n",
    "\n",
    "high_penalty = 50\n",
    "\n",
    "for route in routes.values():  # Assuming routes contains lists of indices\n",
    "    route_indices = route  # Ensure route is a list of integer indices\n",
    "    routing.AddSoftSameVehicleConstraint(route_indices, high_penalty)\n",
    "\n",
    "search_parameters.use_cp_sat = pywrapcp.BOOL_FALSE\n",
    "search_parameters.use_generalized_cp_sat = pywrapcp.BOOL_FALSE\n",
    "search_parameters.guided_local_search_lambda_coefficient = 0.9\n",
    "\n",
    "# search_parameters.fallback_to_cp_sat_size_threshold = 90\n",
    "search_parameters.cheapest_insertion_first_solution_min_neighbors = 39\n",
    "search_parameters.cheapest_insertion_first_solution_neighbors_ratio = 0.9\n",
    "search_parameters.sat_parameters.num_search_workers = 12\n",
    "search_parameters.sat_parameters.linearization_level = 11\n",
    "print(\"data: starting to solve\")\n",
    "\n",
    "\n",
    "class SolutionCallback:\n",
    "    \"\"\"Create a solution callback.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        manager: pywrapcp.RoutingIndexManager,\n",
    "        model: pywrapcp.RoutingModel,\n",
    "        limit: int,\n",
    "    ):\n",
    "        # We need a weak ref on the routing model to avoid a cycle.\n",
    "        self._routing_manager_ref = weakref.ref(manager)\n",
    "        self._routing_model_ref = weakref.ref(model)\n",
    "        self._counter = 0\n",
    "        self._counter_limit = limit\n",
    "        self.objectives = []\n",
    "        self.progress = 0\n",
    "        self.accepted_neighbors = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        # print(f\"ok: {self._routing_model_ref.solver().SearchTrace('abc')}\")\n",
    "        accepted_neighbors = self._routing_model_ref().solver().AcceptedNeighbors()\n",
    "        progress = int(self._routing_model_ref().solver().TopProgressPercent())\n",
    "        objective = int(self._routing_model_ref().CostVar().Value())\n",
    "        branches = self._routing_model_ref().solver().Branches()\n",
    "        model_stamp = self._routing_model_ref().solver().Stamp()\n",
    "        wall_time = self._routing_model_ref().solver().WallTime()\n",
    "        self._routing_model_ref()\n",
    "        if not self.objectives or objective < self.objectives[-1]:\n",
    "            self.objectives.append(objective)\n",
    "            # print_solution(self._routing_manager_ref(), self._routing_model_ref())\n",
    "            self._counter += 1\n",
    "            if self._counter % 5 == 0:\n",
    "                print(\n",
    "                    f\"Solution #{self._counter}  found as: {objective} Progress: {progress}%  Branches: {branches} Accepted Neighbors: {accepted_neighbors} Model Stamp {model_stamp} Wall Time: {wall_time}\"\n",
    "                )\n",
    "\n",
    "# Assuming 'data', 'manager', 'routing', and 'search_parameters' are already defined:\n",
    "solution_callback = SolutionCallback(manager, routing, 19400)\n",
    "routing.AddAtSolutionCallback(solution_callback, True)\n",
    "search_parameters.local_search_operators.use_relocate_neighbors = pywrapcp.BOOL_TRUE\n",
    "# search_parameters.local_search_operators.use_relocate_neighbors = pywrapcp.BOOL_TRUE\n",
    "\n",
    "# Define a large penalty for empty routes\n",
    "\n",
    "\n",
    "solution = routing.SolveWithParameters(search_parameters)\n",
    "\n",
    "\n",
    "solution_limit += 20\n",
    "print(\n",
    "    f\"Objective: {solution.ObjectiveValue()} Reloc Exp Arcs: {relocate_expensive_arcs} Heur Exp Arcs: {heuristic_expensive_arcs} Heur Close Nodes: {heuristic_close_nodes} Prime Time: {seconds_for_prime} Explore Prime: {explore_prime}\"\n",
    ")\n",
    "if best_obj > solution.ObjectiveValue():\n",
    "    best_obj = solution.ObjectiveValue()\n",
    "    # add a row to the best settings dataframe using concat\n",
    "# best_settings = pd.concat([best_settings, pd.DataFrame({\"Objective Value\": [solution.ObjectiveValue()], \"Relocate Expensive Arcs\": [relocate_expensive_arcs], \"Heuristic Expensive Arcs\": [heuristic_expensive_arcs], \"Heuristic Close Nodes\": [heuristic_close_nodes], \"Seconds for Prime\": [seconds_for_prime], \"Explore Prime\": [explore_prime]})])\n",
    "print(\n",
    "    f\"Objective: {solution.ObjectiveValue()} Reloc Exp Arcs: {relocate_expensive_arcs} Heur Exp Arcs: {heuristic_expensive_arcs} Heur Close Nodes: {heuristic_close_nodes} Prime Time: {seconds_for_prime} Explore Prime: {explore_prime}\"\n",
    ")\n",
    "# solution = rSolve.Solve(search_parameters)\n",
    "# trash = print_solution(data, manager, routing, solution, dataset)\n",
    "search_parameters.local_search_operators.use_relocate_neighbors = pywrapcp.BOOL_TRUE\n",
    "\n",
    "filename = \"./data/best_tracker85.csv\"\n",
    "# assign new route days and stops\n",
    "dataset = assign_new_rts(data, manager, routing, solution, dataset)\n",
    "average_new_dist = (\n",
    "    dataset.groupby([\"NEW RT\", \"NEW DAY\", \"NEW STOP\"])[\"NEW DIST\"].sum().quantile(0.95)\n",
    ")\n",
    "# split the routes into days\n",
    "dataset = set_days_for_routes(\n",
    "    dataset,\n",
    "    min_stops=1,\n",
    "    max_stops=10,\n",
    "    distance_threshold=999,  # average_new_dist,\n",
    "    day_factor=12,\n",
    ")\n",
    "_ = combined_tracker_and_statistics(\n",
    "    solution, filename, dataset, data, manager, routing, search_parameters\n",
    ")\n",
    "m.append(create_map_with_accessible_tooltips(dataset))\n",
    "# write solution to csv\n",
    "# create_excel_by_routes(dataset)\n",
    "# routes_by_days_to_html(dataset)\n",
    "print(solution.ObjectiveValue())\n",
    "dataset.to_csv(\"final_dataset.csv\", index=False)\n",
    "# dispay the image final frame png file\n",
    "fname = create_final_route_frame(dataset)  # Display the final frame\n",
    "filenames.append(fname)\n",
    "# final_fig_OG = (create_final_route_frame(dataset_OG)) # Display the original frame\n",
    "print(\"data: done solving\")\n",
    "# append best settings to an existing csv file\n",
    "# best_settings.to_csv(\"best_settings_for_loop.csv\", mode=\"a\", header=False, index=False)\n",
    "# create_map_with_accessible_tooltips(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_route_statistics(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map_with_accessible_tooltips(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = combined_tracker_and_statistics(solution, filename, dataset, data, manager, routing, search_parameters)\n",
    "# m = create_map_with_accessible_tooltips(dataset)\n",
    "# display(m)\n",
    "# routes_by_days_to_html(dataset)\n",
    "generate_route_statistics(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = assign_new_rts(data, manager, routing, solution, dataset)\n",
    "_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = set_days_for_routes(\n",
    "    dataset,\n",
    "    min_stops=1,\n",
    "    max_stops=10,\n",
    "    distance_threshold=999,  # average_new_dist,\n",
    "    day_factor=12,\n",
    ")\n",
    "# Ensure both datasets have a common identifier, set it as index if not already\n",
    "# dataset.set_index('CUST', inplace=True)\n",
    "# sample_route_data.set_index('CUST', inplace=True)\n",
    "\n",
    "# # Update the dataset with the sample data; this updates rows in 'dataset' with those in 'sample_route_data'\n",
    "# # based on the common index ('CUST' here)\n",
    "# dataset.update(sample_route_data)\n",
    "\n",
    "# # Reset index if necessary, to move 'CUST' back to a column\n",
    "# dataset.reset_index(inplace=True)\n",
    "\n",
    "_ = combined_tracker_and_statistics(\n",
    "    solution, filename, dataset, data, manager, routing, search_parameters)\n",
    "m = create_map_with_accessible_tooltips(dataset)\n",
    "# display(m)\n",
    "routes_by_days_to_html(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_per_iter = 19\n",
    "console = Console()\n",
    "explore_coefficient = 100_000_000_000\n",
    "memory_coefficient = 0.7\n",
    "solve_minutes = 1\n",
    "starting_explore_factor = 1.3\n",
    "loops = 100\n",
    "solution_limit = 2\n",
    "fps = 20\n",
    "# create variable rand_bool that is a random boolean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Loop #This is it ### Click ## HERE\n",
    "\n",
    "# import the rich library progress bar\n",
    "from rich.progress import track\n",
    "from rich import print\n",
    "\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from rich.progress import track\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich import box\n",
    "from rich import color\n",
    "from time import sleep\n",
    "import datetime\n",
    "import random\n",
    "from rich.progress import track, Progress, ProgressColumn, TextColumn, SpinnerColumn, TimeElapsedColumn\n",
    "old_objective = solution.ObjectiveValue() + 1\n",
    "\n",
    "gtg = False\n",
    "# save the current time to a variable\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "\n",
    "for i in range(loops):\n",
    "\n",
    "    # solution_limit += 1\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    now = datetime.datetime.now()\n",
    "    # print the current time in a human readable format\n",
    "    now = now.strftime('%I:%M:%S %p')\n",
    "    print(\n",
    "        f\"Starting iteration {i+1} at {now}... Elapsed time: {elapsed_time}  Explore: {explore_coefficient}  Objective: {solution.ObjectiveValue()}\")\n",
    "    # explore_coefficient = 0#10000000000#int(explore_coefficient / starting_explore_factor)\n",
    "    # if memory_coefficient <= 0.92:\n",
    "    #     memory_coefficient += 0.03\n",
    "    # max_retry = 1\n",
    "    # if gtg and solution.ObjectiveValue() == old_objective:\n",
    "    #     seconds_per_iter = int((seconds_per_iter+1)*1.05)\n",
    "    #     for p in range(max_retry):\n",
    "    #         p_start_time = datetime.datetime.now()\n",
    "    #         rand_decimal = random.uniform(0.01, 0.9)\n",
    "    #         rand_exp = random.randint(2, 9)\n",
    "    #         # rand_int = random.randint(100, 1000)*11**rand_exp\n",
    "    #         rand_int = 0#100000000000000\n",
    "    #         search_parameters = set_search_parameters(search_parameters, solve_minutes=1, explore_coefficient=int(rand_int), memory_coefficient=(rand_decimal), log_search=False, seconds_per_iter=27, solution_limit=solution_limit)\n",
    "\n",
    "    #         print(f\"Iteration: {p} Explore: {rand_int} Memory: {rand_decimal} Objective: {old_objective} New Objective: {solution.ObjectiveValue()}\")\n",
    "    #         old_objective = solution.ObjectiveValue()\n",
    "    #         search_parameters.solution_limit = solution_limit\n",
    "    #         solution = routing.SolveFromAssignmentWithParameters(solution, search_parameters)\n",
    "    #         gtg = False\n",
    "    #         p_end_time = datetime.datetime.now()\n",
    "    #         p_duration = p_end_time - p_start_time\n",
    "    #         print(f\"Elapsed time for iteration {i} is {p_duration} with value {solution.ObjectiveValue()}\")\n",
    "    #         if old_objective != solution.ObjectiveValue():\n",
    "    #             # memory_coefficient = rand_decimal\n",
    "    #             # explore_coefficient = rand_int\n",
    "    #             gtg = True\n",
    "    #             break\n",
    "    # else:\n",
    "    # if True:\n",
    "    search_parameters = set_search_parameters(search_parameters, solve_minutes=solve_minutes, explore_coefficient=int(\n",
    "        explore_coefficient), memory_coefficient=memory_coefficient, log_search=False, seconds_per_iter=seconds_per_iter, solution_limit=solution_limit)\n",
    "    old_objective = solution.ObjectiveValue()\n",
    "    # set a limit for the number of solutions find\n",
    "    search_parameters.solution_limit = solution_limit\n",
    "    search_parameters.time_limit.seconds = seconds_per_iter\n",
    "    search_parameters.use_depth_first_search = pywrapcp.BOOL_TRUE\n",
    "\n",
    "    search_parameters.multi_armed_bandit_compound_operator_exploration_coefficient = explore_coefficient\n",
    "    last_obj = solution.ObjectiveValue()\n",
    "    solution = routing.SolveFromAssignmentWithParameters(\n",
    "        solution, search_parameters)\n",
    "    gtg = True\n",
    "    dataset = assign_new_rts(data, manager, routing, solution, dataset)\n",
    "    # dset = combined_tracker_and_statistics(solution, filename, dataset, data, manager, routing, search_parameters)\n",
    "    average_new_dist = (\n",
    "        dataset.groupby([\"NEW RT\", \"NEW DAY\", \"NEW STOP\"])[\"NEW DIST\"]\n",
    "        .sum()\n",
    "        .quantile(0.92)\n",
    "    )\n",
    "    #  Random condition that is True 20% of the time\n",
    "    if random.random() < 0.2:\n",
    "        explore_coefficient = 0\n",
    "    else:\n",
    "        explore_coefficient = 100_000_000_000\n",
    "    # split the routes into days\n",
    "    dataset = set_days_for_routes(\n",
    "        dataset,\n",
    "        min_stops=5,\n",
    "        max_stops=9,\n",
    "        distance_threshold=average_new_dist,\n",
    "        day_factor=4,\n",
    "    )\n",
    "    # if memory_coefficient > .9:\n",
    "    #     memory_coefficient = 0.01\n",
    "    # solve_minutes *= 1.05\n",
    "    # starting_explore_factor += 0.1\n",
    "\n",
    "    # if old_objective == solution.ObjectiveValue():\n",
    "    #     # explore_coefficient *= 4\n",
    "    #     seconds_per_iter = int((seconds_per_iter+1)*1.05)\n",
    "    #     if memory_coefficient >= 0.1:\n",
    "    #         memory_coefficient -= 0.1\n",
    "    # create animation every 4th iter of i\n",
    "    if last_obj != solution.ObjectiveValue():\n",
    "        fname = create_final_route_frame(dataset)  # Display the final frame\n",
    "        filenames.append(fname)\n",
    "    # display(final_fig[i])\n",
    "    if i % 5 == 0:\n",
    "        seconds_per_iter += 2\n",
    "        solution_limit += 1\n",
    "\n",
    "\n",
    "dataset = assign_new_rts(data, manager, routing, solution, dataset)\n",
    "\n",
    "dataset = set_days_for_routes(\n",
    "    dataset,\n",
    "    min_stops=5,\n",
    "    max_stops=8,\n",
    "    distance_threshold=average_new_dist,\n",
    "    day_factor=2,\n",
    ")\n",
    "# _ = combined_tracker_and_statistics(solution, filename, dataset, data, manager, routing, search_parameters)\n",
    "# m = create_map_with_accessible_tooltips(dataset)\n",
    "# display(m)\n",
    "# create_video_from_filenames(filenames)\n",
    "\n",
    "\n",
    "def display_video_in_notebook(video_path):\n",
    "    \"\"\"\n",
    "    Display a video file in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file to display.\n",
    "    \"\"\"\n",
    "    video_html = HTML(f\"\"\"\n",
    "        <video width=\"640\" height=\"480\" controls>\n",
    "          <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "        </video>\n",
    "    \"\"\")\n",
    "    display(video_html)\n",
    "\n",
    "\n",
    "create_video_from_filenames(\n",
    "    filenames, fps=fps, output_filename=f\"rtev{solution.ObjectiveValue()}.mp4\")\n",
    "display_video_in_notebook(f\"rtev{solution.ObjectiveValue()}.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_filenames(\n",
    "    filenames[60:], fps=2, output_filename=f\"rtevakSA{solution.ObjectiveValue()}.mp4\")\n",
    "display_video_in_notebook(f\"rtevakSA{solution.ObjectiveValue()}.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map_with_accessible_tooltips(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"final_dataset5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_filenames(\n",
    "    filenames, fps=5, output_filename=f\"dog{solution.ObjectiveValue()}.mp4\")\n",
    "display_video_in_notebook(f\"dog{solution.ObjectiveValue()}.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_video_from_filenames(filenames, fps=3,)\n",
    "display_video_in_notebook('route_animation.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def clear_image_output_directory(directory):\n",
    "    \"\"\"\n",
    "    Deletes all files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory from which to delete files.\n",
    "    \"\"\"\n",
    "    # Construct a path pattern to match all files in the directory\n",
    "    file_pattern = os.path.join(directory, '*')\n",
    "    # List all files matching the pattern\n",
    "    files = glob.glob(file_pattern)\n",
    "    # Loop over the list of filepaths & remove each file.\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    print(f\"All files removed from {directory}\")\n",
    "\n",
    "\n",
    "# Assuming your output directory is named 'route_snapshots'\n",
    "clear_image_output_directory('route_snapshots')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map_with_accessible_tooltips(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
